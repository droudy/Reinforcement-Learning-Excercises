{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-Armed Bandits Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this multi-armed bandit example, the action values, $q_*(a), a = 1, . . . , 10$ were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action $A_t$ at time $t$, the actual reward $R_t$ was selected from a normal distribution with mean $q_* (A_t)$ and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "class Bandit(object):\n",
    "    \"\"\"10 armed bandit\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_vals = np.random.normal(0, 1, 10)\n",
    "    \n",
    "    \"\"\"Bandit[A] returns the actual reward for action A\"\"\"\n",
    "    def __getitem__(self, action):\n",
    "        action_val = self.action_vals[action]\n",
    "        return action_val + np.random.normal(action_val, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value method using sample averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $Q_n$ and the $n$th reward, $R_n$ , the new average of all $n$ rewards can be computed by: $$Q_{n+1}= Q_n + \\frac{1}{n}[R_n - Q_n]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(object):\n",
    "    \"\"\"An epsilon greedy action value method that uses sample averages\"\"\"\n",
    "    \n",
    "    def __init__(self, Bandit, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.bandit = Bandit()\n",
    "        self.estimated_vals = np.zeros(10)\n",
    "        self.num_a = np.zeros(10)  # num of times that each action has been selected\n",
    "        \n",
    "    def get_action(self):\n",
    "        if(np.random.random() > self.epsilon):  \n",
    "            return np.argmax(self.estimated_vals)\n",
    "        else: \n",
    "            return np.random.randint(0, 10)\n",
    "        \n",
    "    def take_step(self):\n",
    "        action = self.get_action()\n",
    "        reward = self.bandit[action]\n",
    "        self.num_a[action] += 1\n",
    "        error = reward - self.estimated_vals[action]\n",
    "        self.estimated_vals[action] += (1.0 / self.num_a[action]) * error\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any learning method, we can\n",
    "measure its performance and behavior as it improves with experience over 1000 steps\n",
    "interacting with one of the bandit problem. This makes up one run. Repeating this\n",
    "for 2000 independent runs with a different bandit problem, we obtain measures of\n",
    "the learning algorithm’s average behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Returns reward history of a 1000 step run for a given agent\"\"\"   \n",
    "def thousand_step_run(agent):\n",
    "    reward_history = np.zeros(1000)\n",
    "    for i in xrange(1000):\n",
    "        reward_history[i] = agent.take_step()\n",
    "    return reward_history\n",
    "\n",
    "\"\"\"Average reward of 2000 agents each of which made a 1000 step run\"\"\"\n",
    "def avg_reward(AgentClass, *args):\n",
    "    reward_history = np.zeros(1000)\n",
    "    for _ in xrange(2000):\n",
    "        agent = AgentClass(*args)\n",
    "        reward_history = np.add(reward_history, thousand_step_run(agent))\n",
    "    return reward_history / 2000\n",
    "\n",
    "x_axis = [x for x in xrange(1000)]\n",
    "plt.plot(x_axis, avg_reward(EpsilonGreedyAgent, Bandit, .1), label=\"epsilon=.1\")\n",
    "plt.plot(x_axis, avg_reward(EpsilonGreedyAgent, Bandit, .01), label=\"epsilon=.01\")\n",
    "plt.plot(x_axis, avg_reward(EpsilonGreedyAgent, Bandit, 0), label=\"epsilon=0 (greedy)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-value method using a constant step-size parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be: \n",
    "\n",
    "$$Q_{n+1} = Q_n + \\alpha[R_n - Q_n]$$\n",
    "\n",
    "where the step-size parameter α ∈ (0, 1] is constant. This results in $Q_{n+1}$ being a\n",
    "weighted average of past rewards and the initial estimate $Q_1$:\n",
    "\n",
    "$$Q_{n+1} = (1-\\alpha)^nQ_1+\\sum_{i=1}^{n}\\alpha(1-\\alpha)^{n-i}R_i  $$\n",
    "\n",
    "**Exercise 2.3**:\n",
    "Design and conduct an experiment to demonstrate\n",
    "the difficulties that sample-average methods have for nonstationary problems. Use a\n",
    "modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and\n",
    "then take independent random walks. Prepare plots like Figure 2.2 for an action-\n",
    "value method using sample averages, incrementally computed by α = $\\frac{1}{n}$ , and another\n",
    "action-value method using a constant step-size parameter, α = 0.1. Use ε = 0.1 and,\n",
    "if necessary, runs longer than 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NonStationaryBandit(object):\n",
    "    \"\"\"A bandit whos action values for each arm are non-stationary(take independent random walks)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_values = np.ones(10)\n",
    "        \n",
    "    def __getitem__(self, action):\n",
    "        random_steps = [random.uniform(-2, 2) for _ in xrange(10)]\n",
    "        self.action_values = np.add(self.action_values, random_steps)\n",
    "        return self.action_values[action]\n",
    "    \n",
    "    \n",
    "class EpsilonGreedyConstStepSize(EpsilonGreedyAgent):\n",
    "    \"\"\"An epsilon greedy action value method that uses a constant step-size parameter\"\"\"\n",
    "    \n",
    "    def __init__(self, Bandit, epsilon, alpha):\n",
    "        super(EpsilonGreedyConstStepSize, self).__init__(Bandit, epsilon)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def take_step(self):\n",
    "        action = self.get_action()\n",
    "        reward = self.bandit[action]\n",
    "        self.num_a[action] += 1\n",
    "        self.estimated_vals[action] += self.alpha * (reward - self.estimated_vals[action]) \n",
    "        return reward\n",
    "    \n",
    "sample_avg_method_reward = avg_reward(EpsilonGreedyAgent, NonStationaryBandit, .1)\n",
    "const_step_size_reward = avg_reward(EpsilonGreedyConstStepSize, NonStationaryBandit, .1, .1)\n",
    "plt.plot(x_axis, sample_avg_method_reward, label=\"Sample Average Method\")\n",
    "plt.plot(x_axis, const_step_size_reward, label=\"Constant Step Size Parameter\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
